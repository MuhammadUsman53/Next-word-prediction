from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle
import os

app = FastAPI()

# MODEL_FILE = "a:/Assignment/model/next_word_model.h5"
# TOKENIZER_FILE = "a:/Assignment/model/tokenizer.pickle"

# Use relative paths for deployment
import os
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_FILE = os.path.join(BASE_DIR, "../model/next_word_model.h5")
TOKENIZER_FILE = os.path.join(BASE_DIR, "../model/tokenizer.pickle")

SEQUENCE_LENGTH = 5 # Must match training


model = None
tokenizer = None

def load_resources():
    global model, tokenizer
    try:
        print("Loading model...")
        model = tf.keras.models.load_model(MODEL_FILE)
        print("Model loaded.")
        
        print("Loading tokenizer...")
        with open(TOKENIZER_FILE, 'rb') as handle:
            tokenizer = pickle.load(handle)
        print("Tokenizer loaded.")
    except Exception as e:
        print(f"Error loading resources: {e}")
        # In production, we might want to shut down or retry.
        # For now, endpoints will check if model is loaded.

@app.on_event("startup")
async def startup_event():
    load_resources()

class PredictionRequest(BaseModel):
    text: str

@app.post("/predict")
async def predict_next_word(request: PredictionRequest):
    global model, tokenizer
    if model is None or tokenizer is None:
        # Try loading again if failed on startup (optional)
        load_resources()
        if model is None or tokenizer is None:
            raise HTTPException(status_code=503, detail="Model not loaded")

    text = request.text.lower()
    
    # Preprocess
    token_list = tokenizer.texts_to_sequences([text])[0]
    token_list = pad_sequences([token_list], maxlen=SEQUENCE_LENGTH, padding='pre')
    
    # Predict
    predicted = model.predict(token_list, verbose=0)
    predicted_index = np.argmax(predicted, axis=-1)[0]
    
    predicted_word = tokenizer.index_word.get(predicted_index, "")
            
    return {"prediction": predicted_word}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)
